{
 "metadata": {
  "name": "",
  "signature": "sha256:7e0c170a49637e02ea9ecce84e809d1330c2803fcdb4b93a982f1bcc3f0e92aa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Markov Decision Processes\n",
      "\n",
      "***Getting Started with a Simple Example***"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Daisuke Oyama**\n",
      "\n",
      "*Faculty of Economics, University of Tokyo*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook demonstrates via a simple example how to use the `MDP` module."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import numpy as np\n",
      "from mdp import MDP"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## A simple example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us consider the following two-state markov decision process,\n",
      "taken from Puterman (2005), Section 3.1, pp.33-35;\n",
      "see also Example 6.2.1, pp.155-156.\n",
      "\n",
      "* There are two possible states $0$ and $1$.\n",
      "\n",
      "* At state $0$, you may choose either \"stay\", say action $0$, or \"move\", action $1$.\n",
      "\n",
      "* At state $1$, there is no way to move, so that you can only stay, i.e.,\n",
      "  $0$ is the only available action.\n",
      "  (You may alternatively distinguish between the action \"staty\" at state $0$\n",
      "  and that at state $1$, and call the latter action $2$;\n",
      "  but here we choose to refer to the both actions as action $0$.)\n",
      "\n",
      "* At state $0$,\n",
      "  if you choose action $0$ (stay),\n",
      "  then you receive a reward $5$, and\n",
      "  in the next period the state will remain at $0$ with probability $1/2$,\n",
      "  but it moves to $1$ with probability $1/2$.\n",
      "\n",
      "* If you choose action $1$ (move),\n",
      "  then you receive a reward $10$, and\n",
      "  the state in the next period will be $1$ with probability $1$.\n",
      "\n",
      "* At state $1$, where the only action you can take is $0$ (stay),\n",
      "  you receive a reward $-1$, and\n",
      "  the state will remain at $1$ with probability $1$.\n",
      "\n",
      "* You want to maximize the sum of discounted expected reward flows\n",
      "  with discount factor $\\beta \\in (0, 1)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The optimization problem consists of:\n",
      "\n",
      "* the state space: $S = \\{0, 1\\}$;\n",
      "\n",
      "* the action space: $A = \\{0, 1\\}$;\n",
      "\n",
      "* the set of feasible state-action pairs\n",
      "  $\\mathit{SA} = \\{(0, 0), (0, 1), (1, 0)\\} \\subset S \\times A$;\n",
      "\n",
      "* the reward function $r\\colon \\mathit{SA} \\to \\mathbb{R}$, where\n",
      "  $$\n",
      "  r(0, 0) = 5,\\ r(0, 1) = 10,\\ r(1, 0) = -1;\n",
      "  $$\n",
      "\n",
      "* the transition probability function $q \\colon \\mathit{SA} \\to \\Delta(S)$, where\n",
      "  $$\n",
      "  (q(0 | 0, 0), q(1 | 0, 0)) = (1/2, 1/2),\\ \n",
      "  (q(0 | 0, 1), q(1 | 0, 1)) = (0, 1),\\ \n",
      "  (q(0 | 1, 0), q(1 | 1, 0)) = (0, 1);\n",
      "  $$\n",
      "  \n",
      "* the discount factor $\\beta \\in (0, 1)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Belmann equation for this problem is:\n",
      "$$\n",
      "\\begin{aligned}\n",
      "v(0) &= \\max \\left\\{5 + \\beta \\left(\\frac{1}{2} v(0) + \\frac{1}{2} v(1)\\right),\n",
      "                    10 + \\beta v(1)\\right\\}, \\\\\n",
      "v(1) &= (-1) + \\beta v(1).\n",
      "\\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This problem is simple enough to solve by hand:\n",
      "the optimal value function $v^*$ is given by\n",
      "$$\n",
      "\\begin{aligned}\n",
      "&v(0) =\n",
      "\\begin{cases}\n",
      "\\dfrac{5 - 5.5 \\beta}{(1 - 0.5 \\beta) (1 - \\beta)} & \\text{if $\\beta > \\frac{10}{11}$} \\\\\n",
      "\\dfrac{10 - 11 \\beta}{1 - \\beta} & \\text{otherwise},\n",
      "\\end{cases}\\\\\n",
      "&v(1) = -\\frac{1}{1 - \\beta},\n",
      "\\end{aligned}\n",
      "$$\n",
      "and the optimal policy function $\\sigma^*$ is given by\n",
      "$$\n",
      "\\begin{aligned}\n",
      "&\\sigma^*(0) =\n",
      "\\begin{cases}\n",
      "0 & \\text{if $\\beta > \\frac{10}{11}$} \\\\\n",
      "1 & \\text{otherwise},\n",
      "\\end{cases}\\\\\n",
      "&\\sigma^*(1) = 0.\n",
      "\\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def v_star(beta):\n",
      "    v = np.empty(2)\n",
      "    v[1] = -1 / (1 - beta)\n",
      "    if beta > 10/11:\n",
      "        v[0] = (5 - 5.5*beta) / ((1 - 0.5*beta) * (1 - beta))\n",
      "    else:\n",
      "        v[0] = (10 - 11*beta) / (1 - beta)\n",
      "    return v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to solve this optimization problem numerically by using the `MDP` class."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will set $\\beta = 0.95$, for which the anlaytical solution is:\n",
      "$\\sigma^* = (0, 0)$ and"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v_star(beta=0.95)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "array([ -8.57142857, -20.        ])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Formulating the model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two ways to represent the data for instantiating an `MDP` object.\n",
      "Let $n$, $m$, and $L$ denote the numbers of states, actions,\n",
      "and feasbile state-action pairs, respectively;\n",
      "in the above example, $n = 2$, $m = 2$, and $L = 3$.\n",
      "\n",
      "1. `MDP(R, Q, beta)`\n",
      "   \n",
      "   with parameters:\n",
      "   \n",
      "   * $n \\times m$ reward array `R`,\n",
      "   * $n \\times m \\times n$ transition probability array `Q`, and\n",
      "   * discount factor `beta`,\n",
      "   \n",
      "   where `R[s, a]` is the reward for action `a` when the state is `s` and\n",
      "   `Q[s, a, s']` is the probability that the state in the next period is `s'`\n",
      "   when the current state is `s` and the action chosen is `a`.\n",
      "\n",
      "2. `MDP(R, Q, beta, s_indices, a_indices)`\n",
      "\n",
      "   with parameters:\n",
      "   \n",
      "   * length $L$ reward vector `R`,\n",
      "   * $L \\times n$ transition probability array `Q`,\n",
      "   * discount factor `beta`,\n",
      "   * length $L$ array `s_indices`, and\n",
      "   * length $L$ array `a_indices`,\n",
      "   \n",
      "   where the pairs `(s_indices[0], a_indices[0])`, ..., `(s_indices[L-1], a_indices[L-1])`\n",
      "   enumerate feasible state-action pairs, and\n",
      "   `R[i]` is the reward for action `a_indices[i]` when the state is `s_indices[i]` and\n",
      "   `Q[i, s']` is the probability that the state in the next period is `s'`\n",
      "   when the current state is `s_indices[i]` and the action chosen is `a_indices[0]`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Creating an `MDP` instance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us illustrate the two formulations by the simple example at the outset."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Product formulation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This formulation is suitable for simple problems\n",
      "where the set of feasible state-action pairs is naturally represetend\n",
      "by the product $S \\times A$,\n",
      "while any problem can actually be represented in this way\n",
      "by defining the reward `R[s, a]` to be $-\\infty$ when action `a` is infeasible under state `s`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To apply this approach to the current example,\n",
      "we consider the effectively equivalent problem\n",
      "in which at both states $0$ and $1$,\n",
      "both actions $0$ (stay) and $1$ (move) are available,\n",
      "but action $1$ yields a reward $-\\infty$ at state $1$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 2  # Number of states\n",
      "m = 2  # Number of actions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The reward array `R` is a 2-dimensional array of shape `(n, m)`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "R = np.array([[5 , 10     ],\n",
      "              [-1, -np.inf]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "R"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "array([[  5.,  10.],\n",
        "       [ -1., -inf]])"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The transition probability array `Q` is a 3-dimenstional array of shape `(n, m, n)`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q = np.empty((n, m, n))\n",
      "Q[0, 0, :] = 0.5, 0.5\n",
      "Q[0, 1, :] = 0, 1\n",
      "Q[1, 0, :] = 0, 1\n",
      "Q[1, 1, :] = 0.5, 0.5  # Any probability vector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "array([[[ 0.5,  0.5],\n",
        "        [ 0. ,  1. ]],\n",
        "\n",
        "       [[ 0. ,  1. ],\n",
        "        [ 0.5,  0.5]]])"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the transition probabilities for action $(s, a) = (1, 1)$ are arbitrary,\n",
      "since $a = 1$ is infeasible at $s = 1$ in the original problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us set the discount factor $\\beta$ to be $0.95$ ($ > 10/11$):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta = 0.95"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are ready to create an `MDP` instance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdp = MDP(R, Q, beta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### State-action pairs formulation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This formulation takes the set of feasible state-action pairs as is,\n",
      "without treating infeasible actions to be \"feasible but yielding reward $-\\infty$\"."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we have to list all the feasible state-action pairs.\n",
      "For our example, they are: $(s, a) = (0, 0), (0, 1), (1, 0)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have arrays `s_indices` and ` a_indices` of length $3$\n",
      "contain the indices of states and actions, respectively."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 2  # Number of states\n",
      "L = 3  # Number of state-action pairs\n",
      "s_indices = [0, 0, 1]  # State indices\n",
      "a_indices = [0, 1, 0]  # Action indices"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The reward vector `R` is a 1-dimensional array of shape `(L,)`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Rewards for (s, a) = (0, 0), (0, 1), (1, 0), respectively\n",
      "R = np.array([5, 10, -1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The transition probability array `Q` is a 2-dimensional array of shape `(L, n)`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q = np.empty((L, n))\n",
      "\n",
      "# Transition probability vectors\n",
      "Q[0] = 0.5, 0.5  # for (s, a) = (0, 0)\n",
      "Q[1] = 0, 1      # for (s, a) = (0, 1)\n",
      "Q[2] = 0, 1      # for (s, a) = (1, 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "array([[ 0.5,  0.5],\n",
        "       [ 0. ,  1. ],\n",
        "       [ 0. ,  1. ]])"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the discount factor, set $\\beta = 0.95$ as before:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta = 0.95"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now create an `MDP` instance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdp_sa = MDP(R, Q, beta, s_indices, a_indices)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Notes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Importantly, this formulation allows one to represent the transition probability array `Q`\n",
      "as a [`scipy.sparse`](http://docs.scipy.org/doc/scipy/reference/sparse.html) matrix\n",
      "(of any format),\n",
      "which is useful for large and sparse problems."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For example, let us convert the above ndarray `Q` to the Coordinate (coo) format:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.sparse\n",
      "Q = scipy.sparse.coo_matrix(Q)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pass it to `MDP` with the other parameters:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdp_sparse = MDP(R, Q, beta, s_indices, a_indices)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Internally, the matrix `Q` is converted to the Compressed Sparse Row (csr) format:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdp_sparse.Q"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "<3x2 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 4 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdp_sparse.Q.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "array([[ 0.5,  0.5],\n",
        "       [ 0. ,  1. ],\n",
        "       [ 0. ,  1. ]])"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Solving the model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let us solve our MDP model.\n",
      "Currently, `MDP` supports the following solution algorithms:\n",
      "\n",
      "* policy iteration;\n",
      "* value iteration;\n",
      "* modified policy iteration.\n",
      "\n",
      "(The methods are the same across the formulations.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Policy iteration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We solve the model first by policy iteration,\n",
      "which gives the exact solution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w_0 = [0, 0]  # Initial value function\n",
      "res = mdp.solve(method='policy_iteration', w_0=w_0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`res` contains the information about the solution result:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "       mc: Markov chain with transition matrix \n",
        "P = \n",
        "[[ 0.5  0.5]\n",
        " [ 0.   1. ]]\n",
        "        v: array([ -8.57142857, -20.        ])\n",
        " max_iter: 100\n",
        "    sigma: array([0, 0])\n",
        " num_iter: 2\n",
        "   method: 'policy iteration'"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The optimal policy function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res.sigma"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "array([0, 0])"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The optimal value function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res.v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "array([ -8.57142857, -20.        ])"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This coincides with the analytical solution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v_star(beta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "array([ -8.57142857, -20.        ])"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.allclose(res.v, v_star(beta))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The number of iterations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res.num_iter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Verify that the value of the policy `[0, 0]` is actually equal to the optimal value `v`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdp.evaluate_policy(res.sigma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "array([ -8.57142857, -20.        ])"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdp.evaluate_policy(res.sigma) == res.v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "array([ True,  True], dtype=bool)"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`res.mc` is the controlled Markov chain given by the optimal policy `[0, 0]`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res.mc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "Markov chain with transition matrix \n",
        "P = \n",
        "[[ 0.5  0.5]\n",
        " [ 0.   1. ]]"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Value iteration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, solve the model by value iteration,\n",
      "which returns an $\\varepsilon$-optimal solution for a specified value of $\\varepsilon$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epsilon = 1e-2\n",
      "max_iter = 200\n",
      "w_0 = [0, 0]  # Initial value function\n",
      "res1 = mdp.solve(method='value_iteration', w_0=w_0, epsilon=epsilon,\n",
      "                max_iter=max_iter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "       mc: Markov chain with transition matrix \n",
        "P = \n",
        "[[ 0.5  0.5]\n",
        " [ 0.   1. ]]\n",
        "        v: array([ -8.5665053 , -19.99507673])\n",
        "  epsilon: 0.01\n",
        " max_iter: 200\n",
        "    sigma: array([0, 0])\n",
        " num_iter: 162\n",
        "   method: 'value iteration'"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The computed policy function `res1.sigma` is an $\\varepsilon$-optimal policy,\n",
      "and the value function `res1.v` is an $\\varepsilon/2$-approximation\n",
      "of the true optimal value function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.abs(v_star(beta) - res1.v).max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "0.0049232745189442539"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Modified policy iteration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, solve the model by modified policy iteration:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epsilon = 1e-2\n",
      "w_0 = [0, 0]  # Initial value function\n",
      "k = 5  # Number of iterations for policy evaluation\n",
      "\n",
      "res2 = mdp.solve(method='modified_policy_iteration', w_0=w_0, epsilon=epsilon, k=k)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "       mc: Markov chain with transition matrix \n",
        "P = \n",
        "[[ 0.5  0.5]\n",
        " [ 0.   1. ]]\n",
        "  epsilon: 0.01\n",
        "        k: 5\n",
        " max_iter: 100\n",
        "   method: 'modified policy iteration'\n",
        "        v: array([ -8.5702978, -19.9987502])\n",
        "    sigma: array([0, 0])\n",
        " num_iter: 4"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Modified policy function also returns an $\\varepsilon$-optimal policy function\n",
      "and an $\\varepsilon/2$-approximate value function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.abs(v_star(beta) - res2.v).max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "0.0012498048902465086"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Reference\n",
      "\n",
      "* M.L. Puterman,\n",
      "  [*Markov Decision Processes: Discrete Stochastic Dynamic Programming*](http://onlinelibrary.wiley.com/book/10.1002/9780470316887),\n",
      "  Wiley-Interscience, 2005."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    }
   ],
   "metadata": {}
  }
 ]
}