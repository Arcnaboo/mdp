{
 "metadata": {
  "name": "",
  "signature": "sha256:da7e9106566bb73e575695f5f9eb539deddc27ea6eb098f8edc3586507d941b7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Markov Decision Processes\n",
      "\n",
      "***Theoretical background and implementation details***"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Daisuke Oyama**  \n",
      "*Faculty of Economics, University of Tokyo*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook describes the theoretical background and\n",
      "implementation details of the `MDP` class."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Formulation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A (finite) Markov decision process consists of the following components:\n",
      "\n",
      "* finite set of states $S = \\{0, \\ldots, n-1\\}$;\n",
      "* finite set of available actions $A(s)$ for each state $s \\in S$,\n",
      "  with $A = \\bigcup_{s \\in S} A(s) = \\{0, \\ldots, m-1\\}$,\n",
      "  where we denote $\\mathit{SA} = \\{(s, a) \\in S \\times A \\mid a \\in A(s)\\}$,\n",
      "  which is the set of feasible state-action pairs;\n",
      "* reward function $r\\colon \\mathit{SA} \\to \\mathbb{R}$, where\n",
      "  $r(s, a)$ is the reward when the current state is $s$ and the action chosen is $a$;\n",
      "* transition probability function $q\\colon \\mathit{SA} \\to \\Delta(S)$, where\n",
      "  $q(s'|s, a)$ is the probability that the state in the next period is $s'$\n",
      "  when the current state is $s$ and the action chosen is $a$; and\n",
      "* discount factor $\\beta \\in [0, 1)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A *policy function*, or simply *policy*, is a function $\\sigma\\colon S \\to A$;\n",
      "it is *feasible* if it satisfies $\\sigma(s) \\in A(s)$ for all $s \\in S$.\n",
      "Denote the set of all feasible policies by $\\Sigma$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the decision maker follows a policy $\\sigma \\in \\Sigma$, then for each state $s \\in S$,\n",
      "* he receives a reward $r(s, \\sigma(s))$;\n",
      "  write $r_{\\sigma} \\in \\mathbb{R}^S$ for the reward vector for $\\sigma$\n",
      "  (where $r_{\\sigma}(s) = r(s, \\sigma(s))$); and \n",
      "* the probability that the state in the next period is $s'$ is $q(s'|s, \\sigma(s))$;\n",
      "  write $Q_{\\sigma} \\in \\mathbb{R}^{S \\times S}$ for the transition probability matrix for $\\sigma$\n",
      "  (where $Q_{\\sigma}(s, s') = q(s'|s, \\sigma(s))$).\n",
      "\n",
      "Thus, the expected reward at period $t$ when the decision maker follows $\\sigma$ and\n",
      "the initial state (the state at period $t = 0$) is $s$ is given by\n",
      "$(Q_{\\sigma}^t r_{\\sigma})(s)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The objective of the decision maker, in effect, is\n",
      "to choose a feasible policy $\\sigma$\n",
      "that maximizes $v_{\\sigma}(s)$ for each $s \\in S$, where\n",
      "$v_{\\sigma}(s)$ is the discounted sum of expected reward flows from the policy $\\sigma$\n",
      "when the initial state is $s$, i.e.,\n",
      "$$\n",
      "v_{\\sigma}(s) = \\sum_{t=0}^{\\infty} \\beta^t (Q_{\\sigma}^t r_{\\sigma})(s).\n",
      "$$\n",
      "The function $v_{\\sigma}\\colon S \\to \\mathbb{R}$ so defined\n",
      "is called the *policy value function* for the policy $\\sigma$.\n",
      "\n",
      "The *optimal value function*, or simply *value function*,\n",
      "is the function $v^*\\colon S \\to \\mathbb{R}$ given by\n",
      "$$\n",
      "v^*(s) = \\max_{\\sigma \\in \\Sigma} v_{\\sigma}(s) \\quad (s \\in S).\n",
      "$$\n",
      "A feasible policy function $\\sigma$ is *optimal*\n",
      "if $v_{\\sigma}(s) = v^*(s)$ for all $s \\in S$.\n",
      "For $\\varepsilon > 0$,\n",
      "$v$ is an $\\varepsilon$-approximation of $v^*$ if\n",
      "$\\lVert v - v^*\\rVert < \\varepsilon$, and\n",
      "$\\sigma$ is $\\varepsilon$-optimal if\n",
      "$v_{\\sigma}$ is an $\\varepsilon$-approximation of $v^*$,\n",
      "where $\\lVert \\cdot\\rVert$ is the max norm."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##The operators"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is useful to define the following operators:\n",
      "\n",
      "* The *Bellman operator* $T\\colon \\mathbb{R}^S \\to \\mathbb{R}^S$ is defined by\n",
      "  $$\n",
      "  (T v)(s) = \\max_{a \\in A(s)} r(s, a) + \\beta \\sum_{s' \\in S} q(s'|s, a) v(s')\n",
      "  \\quad (s \\in S).\n",
      "  $$\n",
      "\n",
      "* For a policy function $\\sigma$,\n",
      "  the operator $T_{\\sigma}\\colon \\mathbb{R}^S \\to \\mathbb{R}^S$ is defined by\n",
      "  $$\n",
      "  (T_{\\sigma} v)(s) = r(s, \\sigma(s)) + \\beta \\sum_{s' \\in S} q(s'|s, \\sigma(s)) v(s')\n",
      "  \\quad (s \\in S),\n",
      "  $$\n",
      "  or $T_{\\sigma} v = r_{\\sigma} + \\beta Q_{\\sigma} v$.\n",
      "\n",
      "These operators are monotone, and contraction mappings with modulus $\\beta$.\n",
      "For any policy $\\sigma$, its value $v_{\\sigma}$ is the unique fixed point of $T_{\\sigma}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##The Bellman equation and the Principle of Optimality"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The main principle of the theory of dynamic programming is that\n",
      "\n",
      "* the optimal value function $v^*$ is a unique solution to the *Bellman equation*,\n",
      "  $$\n",
      "  v(s) = \\max_{a \\in A(s)} r(s, a) + \\beta \\sum_{s' \\in S} q(s'|s, a) v(s')\n",
      "  \\quad (s \\in S),\n",
      "  $$\n",
      "  or in other words, $v^*$ is the unique fixed point of $T$, and\n",
      "\n",
      "* $\\sigma^*$ is an optimal policy function if and only if it is *$v^*$-greedy*,\n",
      "  i.e., it satisfies\n",
      "  $$\n",
      "  \\sigma^*(s) \\in \\operatorname*{arg\\,max}_{a \\in A(s)} r(s, a)\n",
      "  + \\beta \\sum_{s' \\in S} q(s'|s, \\sigma(s))   v^*(s')\n",
      "  $$\n",
      "  for all $s \\in S$, or $T v^* = T_{\\sigma^*} v^*$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Solution methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `MDP` class currently implements the following solution algorithms:\n",
      "\n",
      "* value iteration;\n",
      "* policy iteration (default);\n",
      "* modified policy iteration.\n",
      "\n",
      "Policy iteration computes an exact optimal policy in finitely many iterations,\n",
      "while value iteration and modified policy iteration return an $\\varepsilon$-optimal policy\n",
      "for a prespecified value of $\\varepsilon$.\n",
      "\n",
      "Value iteration relies on (only) the fact that\n",
      "the Bellman operator $T$ is a contraction mapping\n",
      "and thus iterative application of $T$ to any initial function $v^0$\n",
      "converges to its unique fixed point $v^*$.\n",
      "\n",
      "Policy iteration more closely exploits the particular structure of the problem,\n",
      "where each iteration consists of a policy evaluation step,\n",
      "which computes the value $v_{\\sigma}$ of a policy $\\sigma$\n",
      "by solving the linear equation $v = T_{\\sigma} v$,\n",
      "and a policy improvement step, which computes a $v_{\\sigma}$-greedy policy.\n",
      "\n",
      "Modified policy iteration replaces the policy evaluation step\n",
      "in policy iteration with \"partial policy evaluation\",\n",
      "which computes an approximation of the value of a policy $\\sigma$\n",
      "by iterating $T_{\\sigma}$ for a specified number of times."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below we describe our implementation of these algorithms more in detail."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Value iteration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`MDP.value_iteration(v_init, epsilon, iter_max)`\n",
      "\n",
      "1. Choose any $v^0 \\in \\mathbb{R}^n$, and\n",
      "   specify $\\varepsilon > 0$ and $i_{\\text{max}} \\geq 1$; set $i = 0$.\n",
      "2. Compute $v^{i+1} = T v^i$.\n",
      "3. If $\\lVert v^{i+1} - v^i\\rVert <  [(1 - \\beta) / (2\\beta)] \\varepsilon$ or\n",
      "   $i + 1 = i_{\\text{max}}$, then go to step 4;\n",
      "   otherwise, set $i = i + 1$ and go to step 2.\n",
      "4. Compute a $v^{i+1}$-greedy policy $\\sigma$, and return $v^{i+1}$ and $\\sigma$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given $\\varepsilon > 0$,\n",
      "the value iteration algorithm terminates in a finite number of iterations,\n",
      "and returns an $\\varepsilon/2$-approximation of the optimal value funciton and\n",
      "an $\\varepsilon$-optimal policy function\n",
      "(unless `iter_max` is reached)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Policy iteration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`MDP.policy_iteration(v_init, iter_max)`\n",
      "\n",
      "1. Choose any $v^0 \\in \\mathbb{R}^n$ and compute a $v^0$-greedy policy $\\sigma^0$,\n",
      "   and specify $i_{\\text{max}} \\geq 1$; set $i = 0$.\n",
      "2. [Policy evaluation]\n",
      "   Compute the value $v_{\\sigma^i}$ by solving the equation $v = T_{\\sigma^i} v$.\n",
      "3. [Policy improvement]\n",
      "   Compute a $v_{\\sigma^i}$-greedy policy $\\sigma^{i+1}$;\n",
      "   let $\\sigma^{i+1} = \\sigma^i$ if possible.\n",
      "4. If $\\sigma^{i+1} = \\sigma^i$ or $i + 1 = i_{\\text{max}}$,\n",
      "   then return $v_{\\sigma^i}$ and $\\sigma^{i+1}$;\n",
      "   otherwise, set $i = i + 1$ and go to step 2."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The policy iteration algorithm terminates in a finite number of iterations, and\n",
      "returns an optimal value function and an optimal policy function\n",
      "(unless `iter_max` is reached)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Modified policy iteration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`MDP.modified_policy_iteration(v_init, epsilon, iter_max, k)`\n",
      "\n",
      "1. Choose any $v^0 \\in \\mathbb{R}^n$, and\n",
      "   specify $\\varepsilon > 0$, $i_{\\text{max}} \\geq 1$, and $k \\geq 0$;\n",
      "   set $i = 0$.\n",
      "2. [Policy improvement]\n",
      "   Compute a $v^i$-greedy policy $\\sigma^{i+1}$;\n",
      "   let $\\sigma^{i+1} = \\sigma^i$ if possible (for $i \\geq 1$).\n",
      "3. Compute $u = T v^i$ ($= T_{\\sigma^{i+1}} v^i$).\n",
      "   If $\\mathrm{span}(u - v^i) < [(1 - \\beta) / \\beta] \\varepsilon$, then go to step 5;\n",
      "   otherwise go to step 4.\n",
      "4. [Partial policy evaluation]\n",
      "   Compute $v^{i+1} = (T_{\\sigma^{i+1}})^k u$ ($= (T_{\\sigma^{i+1}})^{k+1} v^i$).\n",
      "   If $i + 1 = i_{\\text{max}}$, then return $v^{i+1}$ and $\\sigma^{i+1}$;\n",
      "   otherwise, set $i = i + 1$ and go to step 2.\n",
      "5. Return\n",
      "   $v = u + [\\beta / (1 - \\beta)] [(\\min(u - v^i) + \\max(u - v^i)) / 2] \\mathbf{1}$\n",
      "   and $\\sigma_{i+1}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given $\\varepsilon > 0$,\n",
      "the modified policy iteration algorithm terminates in a finite number of iterations,\n",
      "and returns an $\\varepsilon/2$-approximation of the optimal value funciton and\n",
      "an $\\varepsilon$-optimal policy function\n",
      "(unless `iter_max` is reached)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Remarks*\n",
      "\n",
      "* Here we employ the termination criterion based on the *span semi-norm*,\n",
      "  where $\\mathrm{span}(z) = \\max(z) - \\min(z)$ for $z \\in \\mathbb{R}^n$.\n",
      "  Since $\\mathrm{span}(T v - v) \\leq 2\\lVert T v - v\\rVert$,\n",
      "  this reaches $\\varepsilon$-optimality faster than the norm-based criterion\n",
      "  as employed in the value iteration above.\n",
      "* Except for the termination criterion,\n",
      "  modified policy is equivalent to value iteration if $k = 0$ and\n",
      "  to policy iteration in the limit as $k \\to \\infty$.\n",
      "* Thus, if one would like to have value iteration with the span-based rule,\n",
      "  run modified policy iteration with $k = 0$.\n",
      "* In returning a value function, our implementation is slightly different from\n",
      "  that by Puterman (2005), Section 6.6.3, pp.201-202, which uses\n",
      "  $u + [\\beta / (1 - \\beta)] \\min(u - v^i) \\mathbf{1}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Illustration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We illustrate the algorithms above\n",
      "by the simple example from Puterman (2005), Section 3.1, pp.33-35."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division, print_function\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from mdp import MDP"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 2  # Number of states\n",
      "m = 2  # Number of actions\n",
      "\n",
      "# Reward array\n",
      "R = [[5, 10],\n",
      "     [-1, -float('inf')]]\n",
      "\n",
      "# Transition probability array\n",
      "Q = [[(0.5, 0.5), (0, 1)],\n",
      "     [(0, 1), (0.5, 0.5)]] # Probabilities in Q[1, 1] are arbitrary\n",
      "\n",
      "# Discount rate\n",
      "beta = 0.95\n",
      "\n",
      "mdp = MDP(R, Q, beta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Analytical solution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigma_star(beta):\n",
      "    sigma = np.empty(2, dtype=int)\n",
      "    sigma[1] = 0\n",
      "    if beta > 10/11:\n",
      "        sigma[0] = 0\n",
      "    else:\n",
      "        sigma[0] = 1\n",
      "    return sigma\n",
      "\n",
      "def v_star(beta):\n",
      "    v = np.empty(2)\n",
      "    v[1] = -1 / (1 - beta)\n",
      "    if beta > 10/11:\n",
      "        v[0] = (5 - 5.5*beta) / ((1 - 0.5*beta) * (1 - beta))\n",
      "    else:\n",
      "        v[0] = (10 - 11*beta) / (1 - beta)\n",
      "    return v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigma_star(beta=beta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "array([0, 0])"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v_star(beta=beta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "array([ -8.57142857, -20.        ])"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Value iteration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Solve the problem by value iteration;\n",
      "see Example 6.3.1, p.164 in Puterman (2005)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epsilon = 1e-2\n",
      "v_init = [0, 0]\n",
      "res_vi = mdp.solve(method='value_iteration', v_init=v_init, epsilon=epsilon)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The number of iterations required to satisfy the termination criterion:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res_vi.num_iter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "162"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The returned value function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res_vi.v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "array([ -8.5665053 , -19.99507673])"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is indeed an $\\varepsilon/2$-approximation of $v^*$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.abs(res_vi.v - v_star(beta=beta)).max() < epsilon"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The returned policy function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res_vi.sigma"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "array([0, 0])"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Value iteration converges very slowly.\n",
      "Let us replicate Table 6.3.1 on p.165:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_reps = 164\n",
      "values = np.empty((num_reps, n))\n",
      "diffs = np.empty(num_reps)\n",
      "spans = np.empty(num_reps)\n",
      "v = np.array([0, 0])\n",
      "\n",
      "values[0] = v\n",
      "diffs[0] = np.nan\n",
      "spans[0] = np.nan\n",
      "\n",
      "for i in range(1, num_reps):\n",
      "    v_new = mdp.bellman_operator(v)\n",
      "    values[i] = v_new\n",
      "    diffs[i] = np.abs(v_new - v).max()\n",
      "    spans[i] = (v_new - v).max() - (v_new - v).min()\n",
      "    v = v_new"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.DataFrame()\n",
      "df[0], df[1], df[2], df[3] = values[:, 0], values[:, 1], diffs, spans\n",
      "df.columns = '$v^i(0)$', '$v^i(1)$', \\\n",
      "             '$\\\\lVert v^i - v^{i-1}\\\\rVert$', '$\\\\mathrm{span}(v^i - v^{i-1})$'\n",
      "\n",
      "iter_nums = pd.Series(list(range(num_reps)), name='$i$')\n",
      "df.index = iter_nums\n",
      "\n",
      "display_nums = \\\n",
      "    list(range(10)) + [10*i for i in range(1, 16)] + [160+i for i in range(4)]\n",
      "df[[0, 1, 2]].iloc[display_nums]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>$v^i(0)$</th>\n",
        "      <th>$v^i(1)$</th>\n",
        "      <th>$\\lVert v^i - v^{i-1}\\rVert$</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>$i$</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>0.000000</td>\n",
        "      <td>0.000000</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>10.000000</td>\n",
        "      <td>-1.000000</td>\n",
        "      <td>10.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>9.275000</td>\n",
        "      <td>-1.950000</td>\n",
        "      <td>0.950000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>8.479375</td>\n",
        "      <td>-2.852500</td>\n",
        "      <td>0.902500</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>7.672766</td>\n",
        "      <td>-3.709875</td>\n",
        "      <td>0.857375</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>6.882373</td>\n",
        "      <td>-4.524381</td>\n",
        "      <td>0.814506</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>6.120046</td>\n",
        "      <td>-5.298162</td>\n",
        "      <td>0.773781</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>5.390395</td>\n",
        "      <td>-6.033254</td>\n",
        "      <td>0.735092</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>4.694642</td>\n",
        "      <td>-6.731591</td>\n",
        "      <td>0.698337</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td>4.032449</td>\n",
        "      <td>-7.395012</td>\n",
        "      <td>0.663420</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>3.402783</td>\n",
        "      <td>-8.025261</td>\n",
        "      <td>0.630249</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td>-1.401710</td>\n",
        "      <td>-12.830282</td>\n",
        "      <td>0.377354</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>30</th>\n",
        "      <td>-4.278653</td>\n",
        "      <td>-15.707225</td>\n",
        "      <td>0.225936</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>40</th>\n",
        "      <td>-6.001185</td>\n",
        "      <td>-17.429757</td>\n",
        "      <td>0.135276</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50</th>\n",
        "      <td>-7.032529</td>\n",
        "      <td>-18.461100</td>\n",
        "      <td>0.080995</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>60</th>\n",
        "      <td>-7.650033</td>\n",
        "      <td>-19.078604</td>\n",
        "      <td>0.048495</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>70</th>\n",
        "      <td>-8.019755</td>\n",
        "      <td>-19.448326</td>\n",
        "      <td>0.029035</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>80</th>\n",
        "      <td>-8.241121</td>\n",
        "      <td>-19.669693</td>\n",
        "      <td>0.017385</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>90</th>\n",
        "      <td>-8.373661</td>\n",
        "      <td>-19.802233</td>\n",
        "      <td>0.010409</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>100</th>\n",
        "      <td>-8.453018</td>\n",
        "      <td>-19.881589</td>\n",
        "      <td>0.006232</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>110</th>\n",
        "      <td>-8.500532</td>\n",
        "      <td>-19.929103</td>\n",
        "      <td>0.003731</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>120</th>\n",
        "      <td>-8.528980</td>\n",
        "      <td>-19.957551</td>\n",
        "      <td>0.002234</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>130</th>\n",
        "      <td>-8.546013</td>\n",
        "      <td>-19.974584</td>\n",
        "      <td>0.001338</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>140</th>\n",
        "      <td>-8.556211</td>\n",
        "      <td>-19.984783</td>\n",
        "      <td>0.000801</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>150</th>\n",
        "      <td>-8.562317</td>\n",
        "      <td>-19.990889</td>\n",
        "      <td>0.000480</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>160</th>\n",
        "      <td>-8.565973</td>\n",
        "      <td>-19.994545</td>\n",
        "      <td>0.000287</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>161</th>\n",
        "      <td>-8.566246</td>\n",
        "      <td>-19.994818</td>\n",
        "      <td>0.000273</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>162</th>\n",
        "      <td>-8.566505</td>\n",
        "      <td>-19.995077</td>\n",
        "      <td>0.000259</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>163</th>\n",
        "      <td>-8.566751</td>\n",
        "      <td>-19.995323</td>\n",
        "      <td>0.000246</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "      $v^i(0)$   $v^i(1)$  $\\lVert v^i - v^{i-1}\\rVert$\n",
        "$i$                                                    \n",
        "0     0.000000   0.000000                           NaN\n",
        "1    10.000000  -1.000000                     10.000000\n",
        "2     9.275000  -1.950000                      0.950000\n",
        "3     8.479375  -2.852500                      0.902500\n",
        "4     7.672766  -3.709875                      0.857375\n",
        "5     6.882373  -4.524381                      0.814506\n",
        "6     6.120046  -5.298162                      0.773781\n",
        "7     5.390395  -6.033254                      0.735092\n",
        "8     4.694642  -6.731591                      0.698337\n",
        "9     4.032449  -7.395012                      0.663420\n",
        "10    3.402783  -8.025261                      0.630249\n",
        "20   -1.401710 -12.830282                      0.377354\n",
        "30   -4.278653 -15.707225                      0.225936\n",
        "40   -6.001185 -17.429757                      0.135276\n",
        "50   -7.032529 -18.461100                      0.080995\n",
        "60   -7.650033 -19.078604                      0.048495\n",
        "70   -8.019755 -19.448326                      0.029035\n",
        "80   -8.241121 -19.669693                      0.017385\n",
        "90   -8.373661 -19.802233                      0.010409\n",
        "100  -8.453018 -19.881589                      0.006232\n",
        "110  -8.500532 -19.929103                      0.003731\n",
        "120  -8.528980 -19.957551                      0.002234\n",
        "130  -8.546013 -19.974584                      0.001338\n",
        "140  -8.556211 -19.984783                      0.000801\n",
        "150  -8.562317 -19.990889                      0.000480\n",
        "160  -8.565973 -19.994545                      0.000287\n",
        "161  -8.566246 -19.994818                      0.000273\n",
        "162  -8.566505 -19.995077                      0.000259\n",
        "163  -8.566751 -19.995323                      0.000246"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On the other hand, the span decreases faster than the norm;\n",
      "the following replicates Table 6.6.1, page 205:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df[[2, 3]].iloc[list(range(1, 13)) + [10*i for i in  range(2, 7)]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>$\\lVert v^i - v^{i-1}\\rVert$</th>\n",
        "      <th>$\\mathrm{span}(v^i - v^{i-1})$</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>$i$</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>10.000000</td>\n",
        "      <td>1.100000e+01</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>0.950000</td>\n",
        "      <td>2.250000e-01</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>0.902500</td>\n",
        "      <td>1.068750e-01</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>0.857375</td>\n",
        "      <td>5.076562e-02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>0.814506</td>\n",
        "      <td>2.411367e-02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>0.773781</td>\n",
        "      <td>1.145399e-02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>0.735092</td>\n",
        "      <td>5.440647e-03</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>0.698337</td>\n",
        "      <td>2.584307e-03</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td>0.663420</td>\n",
        "      <td>1.227546e-03</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>0.630249</td>\n",
        "      <td>5.830844e-04</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>0.598737</td>\n",
        "      <td>2.769651e-04</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>0.568800</td>\n",
        "      <td>1.315584e-04</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td>0.377354</td>\n",
        "      <td>3.409318e-07</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>30</th>\n",
        "      <td>0.225936</td>\n",
        "      <td>1.993445e-10</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>40</th>\n",
        "      <td>0.135276</td>\n",
        "      <td>1.154632e-13</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50</th>\n",
        "      <td>0.080995</td>\n",
        "      <td>0.000000e+00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>60</th>\n",
        "      <td>0.048495</td>\n",
        "      <td>1.776357e-15</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "     $\\lVert v^i - v^{i-1}\\rVert$  $\\mathrm{span}(v^i - v^{i-1})$\n",
        "$i$                                                              \n",
        "1                       10.000000                    1.100000e+01\n",
        "2                        0.950000                    2.250000e-01\n",
        "3                        0.902500                    1.068750e-01\n",
        "4                        0.857375                    5.076562e-02\n",
        "5                        0.814506                    2.411367e-02\n",
        "6                        0.773781                    1.145399e-02\n",
        "7                        0.735092                    5.440647e-03\n",
        "8                        0.698337                    2.584307e-03\n",
        "9                        0.663420                    1.227546e-03\n",
        "10                       0.630249                    5.830844e-04\n",
        "11                       0.598737                    2.769651e-04\n",
        "12                       0.568800                    1.315584e-04\n",
        "20                       0.377354                    3.409318e-07\n",
        "30                       0.225936                    1.993445e-10\n",
        "40                       0.135276                    1.154632e-13\n",
        "50                       0.080995                    0.000000e+00\n",
        "60                       0.048495                    1.776357e-15"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The span-based termination criterion is satisfied when $i = 11$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epsilon * (1-beta) / beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "0.0005263157894736847"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spans[11] < epsilon * (1-beta) / beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In fact, modified policy iteration with $k = 0$ terminates with $11$ iterations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epsilon = 1e-2\n",
      "v_init = [0, 0]\n",
      "k = 0\n",
      "res_mpi_1 = mdp.solve(method='modified_policy_iteration',\n",
      "                      v_init=v_init, epsilon=epsilon, k=k)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res_mpi_1.num_iter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "11"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res_mpi_1.v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "array([ -8.56904799, -19.99736883])"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Policy iteration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If $\\{\\sigma^i\\}$ is the sequence of policies obtained by policy iteration\n",
      "with an initial policy $\\sigma^0$,\n",
      "one can show that $T^i v_{\\sigma^0} \\leq v_{\\sigma^i}$ ($\\leq v^*$),\n",
      "so that the number of iterations required for policy iteration is smaller than\n",
      "that for value iteration at least weakly,\n",
      "and indeed in many cases, the former is significantly smaller than the latter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v_init = [0, 0]\n",
      "res_pi = mdp.solve(method='policy_iteration', v_init=v_init)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res_pi.num_iter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Policy iteration returns the exact optimal value function (up to rounding errors):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.abs(res_pi.v - v_star(beta=beta)).max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "3.5527136788005009e-15"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To look into the iterations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v = np.array([0, 0])\n",
      "sigma = np.array([-1, -1])  # Dummy\n",
      "sigma_new = mdp.compute_greedy(v)\n",
      "i = 0\n",
      "\n",
      "while True:\n",
      "    print('Iterate {0}'.format(i))\n",
      "    print(' value:  {0}'.format(v))\n",
      "    print(' policy: {0}'.format(sigma_new))\n",
      "    if np.array_equal(sigma_new, sigma):\n",
      "        break\n",
      "    sigma[:] = sigma_new\n",
      "    v = mdp.evaluate_policy(sigma)\n",
      "    sigma_new = mdp.compute_greedy(v)\n",
      "    i += 1\n",
      "\n",
      "print('Terminated')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iterate 0\n",
        " value:  [0 0]\n",
        " policy: [1 0]\n",
        "Iterate 1\n",
        " value:  [ -9. -20.]\n",
        " policy: [0 0]\n",
        "Iterate 2\n",
        " value:  [ -8.57142857 -20.        ]\n",
        " policy: [0 0]\n",
        "Terminated\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "See Example 6.4.1, pp.176-177."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Modified policy iteration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The evaluation step in policy iteration\n",
      "which solves the linear equation $v = T_{\\sigma} v$\n",
      "to obtain the policy value $v_{\\sigma}$\n",
      "can be expensive for problems with a large number of states.\n",
      "Modified policy iteration is to reduce the cost of this step\n",
      "by using an approximation of $v_{\\sigma}$ obtained by iteration of $T_{\\sigma}$.\n",
      "The tradeoff is that this approach only computes an $\\varepsilon$-optimal policy,\n",
      "and for small $\\varepsilon$, takes a larger number of iterations than policy iteration\n",
      "(but much smaller than value iteration)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epsilon = 1e-2\n",
      "v_init = [0, 0]\n",
      "k = 6\n",
      "res_mpi = mdp.solve(method='modified_policy_iteration',\n",
      "                    v_init=v_init, epsilon=epsilon, k=k)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res_mpi.num_iter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "4"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To look into the iterations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epsilon = 1e-2\n",
      "v = np.array([0, 0])\n",
      "k = 6\n",
      "i = 0\n",
      "print('Iterate {0}'.format(i))\n",
      "print(' v: {0}'.format(v))\n",
      "\n",
      "sigma = np.empty(n, dtype=int)  # Store the policy function\n",
      "\n",
      "while True:\n",
      "    i += 1\n",
      "    u = mdp.bellman_operator(v, sigma=sigma)\n",
      "    diff = u - v\n",
      "    span = diff.max() - diff.min()\n",
      "    print('Iterate {0}'.format(i))\n",
      "    print(' sigma: {0}'.format(sigma))\n",
      "    print(' T_sigma(v): {0}'.format(u))\n",
      "    print(' span: {0}'.format(span))\n",
      "    if span < epsilon * (1-mdp.beta) / mdp.beta:\n",
      "        v = u + ((diff.max() + diff.min()) / 2) * \\\n",
      "            (mdp.beta / (1 - mdp.beta))\n",
      "        break\n",
      "    mdp.operator_iteration(mdp.T_sigma(sigma), v=u, max_iter=k)\n",
      "    v = u\n",
      "    print(' T_sigma^k+1(v): {0}'.format(v))\n",
      "\n",
      "print('Terminated')\n",
      "print(' sigma: {0}'.format(sigma))\n",
      "print(' v: {0}'.format(v))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iterate 0\n",
        " v: [0 0]\n",
        "Iterate 1\n",
        " sigma: [1 0]\n",
        " T_sigma(v): [ 10.  -1.]\n",
        " span: 11.0\n",
        " T_sigma^k+1(v): [ 4.96674592 -6.03325408]\n",
        "Iterate 2\n",
        " sigma: [0 0]\n",
        " T_sigma(v): [ 4.49340863 -6.73159137]\n",
        " span: 0.225\n",
        " T_sigma^k+1(v): [  1.17973283 -10.24650042]\n",
        "Iterate 3\n",
        " sigma: [0 0]\n",
        " T_sigma(v): [  0.69328539 -10.7341754 ]\n",
        " span: 0.00122754602829\n",
        " T_sigma^k+1(v): [ -1.7602088  -13.18876747]\n",
        "Iterate 4\n",
        " sigma: [0 0]\n",
        " T_sigma(v): [ -2.10076373 -13.5293291 ]\n",
        " span: 6.69719667279e-06\n",
        "Terminated\n",
        " sigma: [0 0]\n",
        " v: [ -8.57137101 -19.99993638]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compare this with the implementation with the norm-based termination rule\n",
      "as described in Example 6.5.1, pp.187-188."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Reference"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* M.L. Puterman,\n",
      "  [*Markov Decision Processes: Discrete Stochastic Dynamic Programming*](http://onlinelibrary.wiley.com/book/10.1002/9780470316887),\n",
      "  Wiley-Interscience, 2005."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    }
   ],
   "metadata": {}
  }
 ]
}